{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wbff-rAfjK-n"
      },
      "source": [
        "# Preprocesamiento de datos\n",
        "\n",
        "El preprocesamiento de datos es un paso fundamental en el aprendizaje automático. Implica limpiar y transformar los datos sin procesar en un formato adecuado para el modelado.\n",
        "\n",
        "## Temas tratados:\n",
        "- Limpieza de datos\n",
        "- Escalado de características\n",
        "- Codificación de variables categóricas\n",
        "- División de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeOk_dONjQCt"
      },
      "source": [
        "## Limpieza de datos\n",
        "\n",
        "La limpieza de datos implica gestionar los valores faltantes, eliminar duplicados y corregir inconsistencias.\n",
        "\n",
        "## Escalado de características\n",
        "\n",
        "El escalado de características garantiza que las características contribuyan de manera equitativa al modelo. Las técnicas comunes incluyen la normalización y la estandarización.\n",
        "\n",
        "## Codificación de variables categóricas\n",
        "\n",
        "Los algoritmos de aprendizaje automático requieren una entrada numérica, por lo que las variables categóricas deben codificarse en números.\n",
        "\n",
        "## División de datos\n",
        "\n",
        "Normalmente dividimos los datos en conjuntos de entrenamiento y prueba para evaluar el rendimiento de nuestro modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybrymCDajZs_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Sample Data\n",
        "data = pd.DataFrame({\n",
        "    'Edad': [22, 25, 47, 52],\n",
        "    'Salario': [15000, 29000, 45000, 50000],\n",
        "    'Compras': ['No', 'Si', 'No', 'Si']\n",
        "})\n",
        "\n",
        "# Limpieza de datos (Data Cleaning)\n",
        "data = data.drop_duplicates()\n",
        "\n",
        "# Escalado de características (Feature Scaling)\n",
        "scaler = StandardScaler()\n",
        "data[['Edad', 'Salario']] = scaler.fit_transform(data[['Edad', 'Salario']])\n",
        "\n",
        "# Codificación de variables categóricas (Encoding Categorical Variables)\n",
        "# sparse is deprecated, use sparse_output instead\n",
        "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
        "encoded_features = encoder.fit_transform(data[['Compras']])\n",
        "encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out())\n",
        "\n",
        "# Combinando Datos (Combining Data)\n",
        "data = pd.concat([data[['Edad', 'Salario']], encoded_df], axis=1)\n",
        "\n",
        "# División de datos (Splitting Data)\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop('Compras_Si', axis=1), data['Compras_Si'], test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iC4-rbYkkcFD"
      },
      "source": [
        "## Ejercicio 1: Limpieza de datos\n",
        "\n",
        "* Cargue un conjunto de datos ejemplo, un archivo CSV) e identifique los valores faltantes. Trate los valores faltantes de manera adecuada.\n",
        "\n",
        "## Ejercicio 2: Escalado de características\n",
        "\n",
        "* Aplique el escalado de características a un conjunto de datos con características numéricas. Compare los resultados antes y después del escalado.\n",
        "\n",
        "## Ejercicio 3: Codificación de variables categóricas\n",
        "\n",
        "* Codifique las variables categóricas en un conjunto de datos e incluya las columnas codificadas en el DataFrame final.\n",
        "\n",
        "## Ejercicio 4: División de datos\n",
        "\n",
        "* Divida un conjunto de datos en conjuntos de entrenamiento y prueba. Asegúrese de que los datos estén divididos de manera que se mantenga la distribución de las clases de destino."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ae5iF3hHi26-"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/file.zip'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Bajar el archivo a la sesion de colab\u001b[39;00m\n\u001b[0;32m     10\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mzip_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     12\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(response\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Extract the ZIP file\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/file.zip'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "# URL donde esta el archivo CSV\n",
        "url = 'https://www.inegi.org.mx/contenidos/programas/enoe/datosabiertos/enoe.zip'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "zip_filename = '/content/file.zip'\n",
        "\n",
        "# Bajar el archivo a la sesion de colab\n",
        "response = requests.get(url)\n",
        "with open(zip_filename, 'wb') as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "# Extract the ZIP file\n",
        "with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "    # Extract all the contents into the /content directory\n",
        "    zip_ref.extractall('/content')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "zgp3ZHwPozCy",
        "outputId": "b31cd6a3-a594-4ddd-d5f4-b43e7a93d64c"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Multiple files found in ZIP file. Only one file per ZIP: ['2017_enoe_csv.zip', '2018_enoe_csv.zip', '2019_enoe_csv.zip']",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-cf8c6dd69e73>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Solución al ejercicio de limpieza de datos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Manejo de valores faltantes mediante la eliminación de filas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Explorar el DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    803\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Zero files found in ZIP file {path_or_buf}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    806\u001b[0m                         \u001b[0;34m\"Multiple files found in ZIP file. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m                         \u001b[0;34mf\"Only one file per ZIP: {zip_names}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Multiple files found in ZIP file. Only one file per ZIP: ['2017_enoe_csv.zip', '2018_enoe_csv.zip', '2019_enoe_csv.zip']"
          ]
        }
      ],
      "source": [
        "# Solución al ejercicio de limpieza de datos\n",
        "data =  pd.read_csv(url)\n",
        "data = data.dropna()  # Manejo de valores faltantes mediante la eliminación de filas\n",
        "\n",
        "# Explorar el DataFrame\n",
        "print(\"Primeras 5 filas:\")\n",
        "print(data.head())\n",
        "\n",
        "print(\"\\nInformación del DataFrame:\")\n",
        "print(data.info())\n",
        "\n",
        "print(\"\\nEstadísticas de resumen:\")\n",
        "print(data.describe(include='all'))\n",
        "\n",
        "print(\"\\nNombres de columnas:\")\n",
        "print(data.columns)\n",
        "\n",
        "print(\"\\nTipos de datos:\")\n",
        "print(data.dtypes)\n",
        "\n",
        "print(\"\\nValores faltantes:\")\n",
        "print(data.isnull().sum())\n",
        "\n",
        "print(\"\\nValores únicos en una columna (p. ej., 'columna_de_ejemplo'):\")\n",
        "print(data['Unidad_involucrada'].unique())\n",
        "\n",
        "print(\"\\nRecuentos de valores en una columna (p. ej., 'columna_de_ejemplo'):\")\n",
        "print(data['Unidad_involucrada'].value_counts())\n",
        "\n",
        "print(\"\\nNúmero de filas duplicadas:\")\n",
        "print(data.duplicated().sum())\n",
        "\n",
        "print(\"\\nMuestra aleatoria de filas:\")\n",
        "print(data.sample(5))\n",
        "\n",
        "print(\"\\nForma del DataFrame:\")\n",
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxypaX3lo3mz"
      },
      "outputs": [],
      "source": [
        "# Solución al ejercicio de escalamiento de características\n",
        "data[['Feature1', 'Feature2']] = scaler.fit_transform(data[['Feature1', 'Feature2']])\n",
        "\n",
        "# Solución al ejercicio de codificación de variables categóricas\n",
        "encoded_df = encoder.fit_transform(data[['Dano_agraviado']])\n",
        "data = pd.concat([data, pd.DataFrame(encoded_df, columns=encoder.get_feature_names_out())], axis=1)\n",
        "\n",
        "# Solución al ejercicio de división de datos\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop('target', axis=1), data['target'], test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2uRy8KElG5t"
      },
      "outputs": [],
      "source": [
        "from ipywidgets import interact\n",
        "\n",
        "def preprocess_data(option):\n",
        "    if option == 'Standardize':\n",
        "        data[['Age', 'Salary']] = scaler.fit_transform(data[['Age', 'Salary']])\n",
        "    elif option == 'Normalize':\n",
        "        data[['Age', 'Salary']] = data[['Age', 'Salary']] / data[['Age', 'Salary']].max()\n",
        "\n",
        "interact(preprocess_data, option=['Standardize', 'Normalize'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
